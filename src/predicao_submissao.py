"""
HACKATHON CGDF - CATEGORIA ACESSO √Ä INFORMA√á√ÉO
Script de Predi√ß√£o para Submiss√£o

Este script classifica pedidos de acesso √† informa√ß√£o como:
- P√öBLICO (classe 0): N√£o cont√©m dados pessoais
- N√ÉO P√öBLICO (classe 1): Cont√©m dados pessoais

A CGDF usar√° este script para avaliar o modelo no conjunto de controle.
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from scipy.sparse import hstack
import sys
import os
import warnings
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Caminho base do projeto (pasta onde est√° este script)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(os.path.dirname(BASE_DIR), 'models')


# ============================================================================
# FUN√á√ïES DE EXTRA√á√ÉO DE FEATURES (Mesmas do treinamento)
# ============================================================================

def extrair_features_adicionais(texto):
    """
    Extrai features baseadas em regras para identificar dados pessoais.
    
    Args:
        texto (str): Texto do pedido de acesso √† informa√ß√£o
        
    Returns:
        dict: Dicion√°rio com 9 features num√©ricas
    """
    import re
    texto = str(texto).lower()
    
    features = {}
    
    # Padr√µes que indicam dados pessoais
    features['tem_cpf'] = 1 if re.search(r'\d{3}\.?\d{3}\.?\d{3}-?\d{2}', texto) else 0
    features['tem_matricula'] = 1 if re.search(r'matr[√≠i]cula\s*:?\s*\d+', texto) else 0
    features['tem_processo'] = 1 if re.search(r'processo\s*:?\s*\d+', texto) else 0
    features['tem_nome_proprio'] = 1 if any(palavra in texto for palavra in ['meu nome', 'minha', 'meu']) else 0
    
    # Palavras-chave relacionadas a dados pessoais
    palavras_pessoais = ['cadastro', 'prontu√°rio', 'laudo', 'exame', 'atestado', 
                         'ficha', 'dados pessoais', 'meus dados', 'minhas informa√ß√µes',
                         'meu processo', 'minha situa√ß√£o', 'companheiro', 'familiar']
    features['palavras_pessoais'] = sum(1 for p in palavras_pessoais if p in texto)
    
    # Pronomes possessivos (forte indicador de dados pessoais)
    pronomes = ['meu', 'minha', 'meus', 'minhas']
    features['pronomes_possessivos'] = sum(texto.count(p) for p in pronomes)
    
    # Verbos em primeira pessoa
    verbos_primeira_pessoa = ['solicito', 'preciso', 'gostaria', 'quero', 'estou']
    features['verbos_primeira_pessoa'] = sum(1 for v in verbos_primeira_pessoa if v in texto)
    
    # Tamanho do texto
    features['tamanho_texto'] = len(texto)
    features['num_palavras'] = len(texto.split())
    
    return features


def extrair_embeddings_dual_bert(textos):
    """
    Extrai embeddings de 2 modelos BERT diferentes.
    
    Modelos utilizados:
    - BERTimbau (neuralmind/bert-base-portuguese-cased): 768 dimens√µes
    - DistilBERT PT (adalbertojunior/distilbert-portuguese-cased): 768 dimens√µes
    Total: 1536 dimens√µes concatenadas
    
    Args:
        textos (list): Lista de textos a processar
        
    Returns:
        numpy.ndarray: Array com shape (n_textos, 1536)
    """
    print(f"Extraindo embeddings BERT para {len(textos)} textos...")
    
    modelos = [
        "neuralmind/bert-base-portuguese-cased",
        "adalbertojunior/distilbert-portuguese-cased"
    ]
    
    all_embeddings = []
    
    for modelo_nome in modelos:
        tokenizer = AutoTokenizer.from_pretrained(modelo_nome)
        modelo = AutoModel.from_pretrained(modelo_nome)
        modelo.to(device)
        modelo.eval()
        
        embeddings = []
        batch_size = 16
        
        with torch.no_grad():
            for i in range(0, len(textos), batch_size):
                batch = textos[i:i+batch_size]
                inputs = tokenizer(batch, padding=True, truncation=True, 
                                 max_length=96, return_tensors='pt')
                inputs = {k: v.to(device) for k, v in inputs.items()}
                outputs = modelo(**inputs)
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.extend(cls_embeddings)
        
        all_embeddings.append(np.array(embeddings))
    
    combined = np.hstack(all_embeddings)
    print(f"‚úì Embeddings extra√≠dos: {combined.shape}")
    return combined


# ============================================================================
# FUN√á√ÉO PRINCIPAL DE PREDI√á√ÉO
# ============================================================================

def prever(arquivo_entrada, arquivo_saida, coluna_texto='Texto Mascarado', coluna_id='ID'):
    """
    Realiza predi√ß√µes no conjunto de controle da CGDF.
    
    CLASSIFICA√á√ÉO:
    - Classe 0 = P√öBLICO (n√£o cont√©m dados pessoais)
    - Classe 1 = N√ÉO P√öBLICO (cont√©m dados pessoais)
    
    Args:
        arquivo_entrada (str): Caminho do arquivo Excel com os dados de teste
        arquivo_saida (str): Caminho onde salvar as predi√ß√µes
        coluna_texto (str): Nome da coluna com o texto dos pedidos (padr√£o: 'Texto Mascarado')
        coluna_id (str): Nome da coluna com ID (padr√£o: 'ID')
    
    Returns:
        pd.DataFrame: DataFrame com as predi√ß√µes
    """
    print("="*70)
    print("PREDI√á√ÉO - HACKATHON CGDF LAI")
    print("Classifica√ß√£o: P√öBLICO vs. N√ÉO P√öBLICO")
    print("="*70)
    
    # 1. Carregar dados de teste
    print(f"\n1. Carregando dados: {arquivo_entrada}")
    
    if not os.path.exists(arquivo_entrada):
        print(f"   ‚úó ERRO: Arquivo n√£o encontrado: {arquivo_entrada}")
        print(f"   Certifique-se de que o arquivo existe no caminho especificado.")
        return
    
    df_teste = pd.read_excel(arquivo_entrada)
    print(f"   ‚úì {len(df_teste)} registros carregados")
    
    # Verificar se coluna existe
    if coluna_texto not in df_teste.columns:
        print(f"   ‚úó ERRO: Coluna '{coluna_texto}' n√£o encontrada")
        print(f"   Colunas dispon√≠veis: {df_teste.columns.tolist()}")
        return
    
    # 2. Carregar modelos treinados
    print("\n2. Carregando modelos...")
    
    # Tentar carregar da pasta models/ primeiro, depois da raiz
    caminhos_modelo = [
        os.path.join(MODELS_DIR, 'modelo_otimizado_maximo.pkl'),
        os.path.join(BASE_DIR, '..', 'models', 'modelo_otimizado_maximo.pkl'),
        'modelo_otimizado_maximo.pkl',
        os.path.join(BASE_DIR, '..', 'modelo_otimizado_maximo.pkl')
    ]
    
    caminhos_vectorizer = [
        os.path.join(MODELS_DIR, 'vectorizer_otimizado.pkl'),
        os.path.join(BASE_DIR, '..', 'models', 'vectorizer_otimizado.pkl'),
        'vectorizer_otimizado.pkl',
        os.path.join(BASE_DIR, '..', 'vectorizer_otimizado.pkl')
    ]
    
    caminhos_config = [
        os.path.join(MODELS_DIR, 'config_otimizado.pkl'),
        os.path.join(BASE_DIR, '..', 'models', 'config_otimizado.pkl'),
        'config_otimizado.pkl',
        os.path.join(BASE_DIR, '..', 'config_otimizado.pkl')
    ]
    
    # Carregar modelo
    modelo = None
    for caminho in caminhos_modelo:
        if os.path.exists(caminho):
            modelo = joblib.load(caminho)
            print(f"   ‚úì Modelo carregado de: {caminho}")
            break
    
    if modelo is None:
        print(f"   ‚úó ERRO: modelo_otimizado_maximo.pkl n√£o encontrado")
        print(f"   Procurado em: {caminhos_modelo[0]}")
        return
    
    # Carregar vectorizer
    vectorizer = None
    for caminho in caminhos_vectorizer:
        if os.path.exists(caminho):
            vectorizer = joblib.load(caminho)
            print(f"   ‚úì Vectorizer carregado")
            break
    
    if vectorizer is None:
        print(f"   ‚úó ERRO: vectorizer_otimizado.pkl n√£o encontrado")
        return
    
    # Carregar config
    config = None
    for caminho in caminhos_config:
        if os.path.exists(caminho):
            config = joblib.load(caminho)
            print(f"   ‚úì Config carregado")
            break
    
    if config is None:
        print(f"   ‚úó ERRO: config_otimizado.pkl n√£o encontrado")
        return
    
    threshold = config.get('threshold', 0.5)
    print(f"   ‚úì Threshold: {threshold:.2f}")
    
    # 3. Extrair features
    print("\n3. Extraindo features...")
    
    # 3a. Embeddings BERT
    textos = df_teste[coluna_texto].astype(str).tolist()
    embeddings = extrair_embeddings_dual_bert(textos)
    
    # 3b. TF-IDF
    print("   Extraindo TF-IDF...")
    tfidf_features = vectorizer.transform(df_teste[coluna_texto].astype(str))
    
    # 3c. Features tradicionais
    print("   Extraindo features tradicionais...")
    features_trad = pd.DataFrame([extrair_features_adicionais(t) for t in textos])
    
    # 3d. Combinar tudo
    print("   Combinando features...")
    X_final = hstack([tfidf_features, embeddings, features_trad.values])
    print(f"   ‚úì {X_final.shape[1]} features combinadas")
    
    # 4. Fazer predi√ß√µes
    print("\n4. Realizando predi√ß√µes...")
    y_proba = modelo.predict_proba(X_final)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)
    
    print(f"   ‚úì Predi√ß√µes conclu√≠das")
    
    # 5. Preparar resultado
    print("\n5. Preparando arquivo de sa√≠da...")
    
    # Criar DataFrame de sa√≠da
    df_resultado = pd.DataFrame()
    
    # Adicionar ID se existir
    if coluna_id in df_teste.columns:
        df_resultado[coluna_id] = df_teste[coluna_id]
    else:
        df_resultado['ID'] = range(1, len(df_teste) + 1)
    
    # Adicionar texto original (para facilitar verifica√ß√£o)
    df_resultado['Texto Mascarado'] = df_teste[coluna_texto]
    
    # CLASSIFICA√á√ÉO NUM√âRICA (0 ou 1)
    df_resultado['Classifica√ß√£o'] = y_pred
    
    # CLASSIFICA√á√ÉO TEXTUAL (P√öBLICO ou N√ÉO P√öBLICO)
    df_resultado['Status'] = df_resultado['Classifica√ß√£o'].map({
        0: 'P√öBLICO',
        1: 'N√ÉO P√öBLICO'
    })
    
    # EXPLICA√á√ÉO
    df_resultado['Justificativa'] = df_resultado['Classifica√ß√£o'].map({
        0: 'N√£o cont√©m dados pessoais',
        1: 'Cont√©m dados pessoais'
    })
    
    # Probabilidades e confian√ßa (para an√°lise)
    df_resultado['Probabilidade_Dados_Pessoais'] = y_proba
    df_resultado['Confian√ßa'] = np.maximum(y_proba, 1 - y_proba)
    
    # 6. Salvar resultado
    # Criar diret√≥rio de sa√≠da se n√£o existir
    output_dir = os.path.dirname(arquivo_saida)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"   ‚úì Diret√≥rio criado: {output_dir}")
    
    df_resultado.to_excel(arquivo_saida, index=False)
    print(f"   ‚úì Resultado salvo: {arquivo_saida}")
    
    # 7. Estat√≠sticas
    print("\n" + "="*70)
    print("ESTAT√çSTICAS DAS PREDI√á√ïES")
    print("="*70)
    print(f"\nTotal de pedidos classificados: {len(df_resultado)}")
    
    print(f"\n{'Status':<20} {'Quantidade':<12} {'Percentual'}")
    print("-"*50)
    for status in ['P√öBLICO', 'N√ÉO P√öBLICO']:
        qtd = (df_resultado['Status'] == status).sum()
        pct = (qtd / len(df_resultado)) * 100
        print(f"{status:<20} {qtd:<12} {pct:>6.1f}%")
    
    print(f"\nConfian√ßa m√©dia das predi√ß√µes: {df_resultado['Confian√ßa'].mean():.2%}")
    
    alta_confianca = (df_resultado['Confian√ßa'] > 0.8).sum()
    print(f"Predi√ß√µes com alta confian√ßa (>80%): {alta_confianca}/{len(df_resultado)} ({alta_confianca/len(df_resultado):.1%})")
    
    print("\n" + "="*70)
    print("LEGENDA:")
    print("  Classifica√ß√£o 0 = P√öBLICO = N√£o cont√©m dados pessoais")
    print("  Classifica√ß√£o 1 = N√ÉO P√öBLICO = Cont√©m dados pessoais")
    print("="*70)
    
    print("\n‚úÖ PREDI√á√ÉO CONCLU√çDA COM SUCESSO!")
    print(f"   Arquivo gerado: {arquivo_saida}")
    print("="*70)
    
    return df_resultado


# ============================================================================
# INTERFACE DE LINHA DE COMANDO
# ============================================================================

def main():
    """
    Ponto de entrada do script.
    Aceita argumentos de linha de comando para facilitar automa√ß√£o.
    """
    if len(sys.argv) < 2:
        print("="*70)
        print("HACKATHON CGDF - SCRIPT DE PREDI√á√ÉO")
        print("Classifica√ß√£o: P√öBLICO vs. N√ÉO P√öBLICO")
        print("="*70)
        print("\nüìã USO RECOMENDADO:")
        print("  python src/predicao_submissao.py data/input/<arquivo.xlsx> data/output/<resultado.xlsx>")
        print("\nüìù EXEMPLO:")
        print("  python src/predicao_submissao.py data/input/teste_cgdf.xlsx data/output/resultado.xlsx")
        print("\nüí° USO ALTERNATIVO (caminhos personalizados):")
        print("  python src/predicao_submissao.py <caminho/entrada.xlsx> <caminho/saida.xlsx>")
        print("\n‚ö†Ô∏è  OBSERVA√á√ïES:")
        print("  - Se n√£o especificar arquivo de sa√≠da, ser√° 'predicoes.xlsx'")
        print("  - O arquivo de entrada deve ter a coluna 'Texto Mascarado'")
        print("  - Recomenda-se colocar arquivo de teste em data/input/")
        print("  - Os resultados ser√£o salvos em data/output/")
        print("\nüìä CLASSIFICA√á√ÉO:")
        print("  0 = P√öBLICO (n√£o cont√©m dados pessoais)")
        print("  1 = N√ÉO P√öBLICO (cont√©m dados pessoais)")
        print("="*70)
        sys.exit(1)
    
    arquivo_entrada = sys.argv[1]
    arquivo_saida = sys.argv[2] if len(sys.argv) > 2 else 'predicoes.xlsx'
    
    prever(arquivo_entrada, arquivo_saida)


if __name__ == "__main__":
    main()